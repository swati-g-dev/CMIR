{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# seed for Python's random library\n",
    "random.seed(SEED)\n",
    "\n",
    "# setting seed for NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# setting seed for PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  \n",
    "\n",
    "# Make PyTorch operations deterministic\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a seed for Python's hash function to control for hash randomization\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing dependencies & import libraries\n",
    "%pip install python-terrier sentence-transformers tqdm pandas\n",
    "\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-21\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parsing TREC format files\n",
    "\n",
    "def parse_trec_corpus(filepath):\n",
    "    corpus = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        docs = re.findall(r\"<DOC>(.*?)</DOC>\", content, re.S)\n",
    "        for doc in docs:\n",
    "            docno = re.search(r\"<DOCNO>(.*?)</DOCNO>\", doc).group(1).strip()\n",
    "            body_match = re.search(r\"<BODY>(.*?)</BODY>\", doc, re.S)\n",
    "            body = body_match.group(1).strip() if body_match else \"\"\n",
    "            corpus.append({\"docno\": docno, \"text\": body})\n",
    "    return corpus\n",
    "\n",
    "def parse_trec_queries(filepath):\n",
    "    queries = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        tops = re.findall(r\"<top>(.*?)</top>\", content, re.S)\n",
    "        for top in tops:\n",
    "            qid = re.search(r\"<num>(.*?)</num>\", top).group(1).strip()\n",
    "            title = re.search(r\"<title>(.*?)</title>\", top, re.S).group(1).strip()\n",
    "            queries.append({\"qid\": qid, \"query\": title})\n",
    "    return queries\n",
    "\n",
    "def parse_qrels(filepath):\n",
    "    qrels = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 4:\n",
    "                qid, _, docno, label = parts\n",
    "                qrels.append({\"qid\": qid, \"docno\": docno, \"label\": int(label)})\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb799506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "corpus = parse_trec_corpus(\"../data/Baseline_Corpus.trec\")\n",
    "train_queries = parse_trec_queries(\"../data/Train_query.trec\")\n",
    "test_queries = parse_trec_queries(\"../data/Test_query.trec\")\n",
    "qrels = parse_qrels(\"../data/QRels_Train.txt\")\n",
    "\n",
    "print(f\"Corpus size: {len(corpus)}\")\n",
    "print(f\"Train queries: {len(train_queries)}, Test queries: {len(test_queries)}\")\n",
    "print(f\"Qrels entries: {len(qrels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 Index\n",
    "index_path = \"./index\"\n",
    "\n",
    "# ensuring directory exists and is writable\n",
    "index_path = os.path.abspath(index_path)\n",
    "os.makedirs(index_path, exist_ok=True)\n",
    "\n",
    "indexer = pt.IterDictIndexer(index_path)\n",
    "indexref = indexer.index(({\"docno\": d[\"docno\"], \"text\": d[\"text\"]} for d in corpus))\n",
    "\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\", num_results=100, controls={\"parse.controls\": \"false\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d95a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Reranker\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "def rerank_with_bert(query, docs, top_k=10):\n",
    "    query_emb = model.encode(query, convert_to_tensor=True)\n",
    "    doc_texts = [doc[\"text\"] for doc in docs]\n",
    "    doc_embs = model.encode(doc_texts, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_emb, doc_embs)[0].cpu().numpy()\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    reranked_docs = []\n",
    "    for rank, (doc, score) in enumerate(ranked[:top_k], 1):\n",
    "        reranked_docs.append({\n",
    "            \"docno\": doc[\"docno\"],\n",
    "            \"score\": float(score),\n",
    "            \"rank\": rank\n",
    "        })\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve & rerank\n",
    "bm25_reranked_results = []\n",
    "doc_lookup = {d[\"docno\"]: d[\"text\"] for d in corpus}  # for BERT reranker\n",
    "\n",
    "def clean_query(q):\n",
    "    return re.sub(r\"[^\\w\\s]\", \" \", q)\n",
    "\n",
    "for q in tqdm(train_queries, desc=\"Processing queries\"):\n",
    "    qid = q[\"qid\"]\n",
    "    raw_query = q[\"query\"]\n",
    "    query = clean_query(raw_query)  \n",
    "\n",
    "    bm25_res = bm25.search(query).to_dict('records')\n",
    "\n",
    "    docs_with_text = []\n",
    "    for res in bm25_res:\n",
    "        if res[\"docno\"] in doc_lookup:\n",
    "            docs_with_text.append({\"docno\": res[\"docno\"], \"text\": doc_lookup[res[\"docno\"]]})\n",
    "\n",
    "    if not docs_with_text:\n",
    "        continue\n",
    "\n",
    "    reranked = rerank_with_bert(query, docs_with_text, top_k=10)\n",
    "\n",
    "    for rank, r in enumerate(reranked, start=1):\n",
    "        bm25_reranked_results.append({\n",
    "            \"qid\": qid,\n",
    "            \"docno\": r[\"docno\"],\n",
    "            \"rank\": rank,\n",
    "            \"score\": r[\"score\"]\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(bm25_reranked_results)\n",
    "print(f\"Results DataFrame size: {results_df.shape}\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_df = pd.DataFrame(qrels)\n",
    "\n",
    "print(qrels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Calculations (MAP, nDCG@10, P@10, MRR)\n",
    "\n",
    "def dcg(scores):\n",
    "    return sum(rel / np.log2(idx + 2) for idx, rel in enumerate(scores))\n",
    "\n",
    "def ndcg(ranked_list, ground_truth, k=10):\n",
    "    rels = [ground_truth.get(doc, 0) for doc in ranked_list[:k]]\n",
    "    ideal_rels = sorted(ground_truth.values(), reverse=True)[:k]\n",
    "    return dcg(rels) / dcg(ideal_rels) if ideal_rels else 0.0\n",
    "\n",
    "def precision_at_k(ranked_list, ground_truth, k):\n",
    "    rels = [1 if ground_truth.get(doc, 0) > 0 else 0 for doc in ranked_list[:k]]\n",
    "    return sum(rels) / k\n",
    "\n",
    "def average_precision(ranked_list, ground_truth):\n",
    "    hits = 0\n",
    "    sum_precisions = 0.0\n",
    "    for i, doc in enumerate(ranked_list):\n",
    "        if ground_truth.get(doc, 0) > 0:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / (i + 1)\n",
    "    return sum_precisions / hits if hits > 0 else 0.0\n",
    "\n",
    "def reciprocal_rank(ranked_list, ground_truth):\n",
    "    for i, doc in enumerate(ranked_list):\n",
    "        if ground_truth.get(doc, 0) > 0:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "metrics = {\"map\": [], \"ndcg\": [], \"P@5\": [], \"P@10\": [], \"recip_rank\": []}\n",
    "\n",
    "for qid in results_df[\"qid\"].unique():\n",
    "    ranked_docs = results_df[results_df[\"qid\"] == qid].sort_values(by=\"score\", ascending=False)[\"docno\"].tolist()\n",
    "    gt = qrels_df[qrels_df[\"qid\"] == qid].set_index(\"docno\")[\"label\"].to_dict()\n",
    "\n",
    "    metrics[\"map\"].append(average_precision(ranked_docs, gt))\n",
    "    metrics[\"ndcg\"].append(ndcg(ranked_docs, gt, k=10))\n",
    "    metrics[\"P@10\"].append(precision_at_k(ranked_docs, gt, k=10))\n",
    "    metrics[\"recip_rank\"].append(reciprocal_rank(ranked_docs, gt))\n",
    "    metrics[\"P@5\"].append(precision_at_k(ranked_docs, gt, k=5))\n",
    "\n",
    "\n",
    "print(\"MAP:\", np.mean(metrics[\"map\"]))\n",
    "print(\"nDCG@10:\", np.mean(metrics[\"ndcg\"]))\n",
    "print(\"P@10:\", np.mean(metrics[\"P@10\"]))\n",
    "print(\"P@5:\", np.mean(metrics[\"P@5\"]))\n",
    "print(\"MRR:\", np.mean(metrics[\"recip_rank\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61439f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final results for submission (qid, docno, rank, score)\n",
    "\n",
    "# Sort results by qid, then by score descending\n",
    "results_df = results_df.sort_values(\n",
    "    by=[\"qid\", \"score\"], \n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "final_df = results_df[[\"qid\", \"docno\", \"rank\", \"score\"]]\n",
    "\n",
    "# Save as CSV in current working directory\n",
    "output_path = \"../output/bm25_bert_submission.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Submission file saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb313e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
